# lightning.pytorch==2.2.2
seed_everything: 0
trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: null
  logger:
    class_path: lightning.pytorch.loggers.NeptuneLogger
    init_args:
      api_key: null
      project: lua/Pretrain-TFT-MBI
      name: ''
      run: null
      log_model_checkpoints: true
      prefix: ''
    dict_kwargs:
      dependencies: infer
  callbacks:
  - class_path: lightning.pytorch.callbacks.EarlyStopping
    init_args:
      monitor: validation/loss
      min_delta: 0.0
      patience: 10
      verbose: false
      mode: min
      strict: true
      check_finite: true
      stopping_threshold: null
      divergence_threshold: null
      check_on_train_epoch_end: null
      log_rank_zero_only: false
  - class_path: transformertf.callbacks.SetOptimizerLRCallback
    init_args:
      lr_file: /tmp/lr.txt
      'on': step
      to: null
  - class_path: transformertf.callbacks.LogHparamsCallback
    init_args:
      monitor: validation/loss
      mode: min
  - class_path: lightning.pytorch.callbacks.RichProgressBar
    init_args:
      refresh_rate: 1
      leave: false
      theme:
        description: white
        progress_bar: '#6206E0'
        progress_bar_finished: '#6206E0'
        progress_bar_pulse: '#6206E0'
        batch_progress: white
        time: grey54
        processing_speed: grey70
        metrics: white
        metrics_text_delimiter: ' '
        metrics_format: .2e
      console_kwargs: null
  fast_dev_run: false
  max_epochs: 50
  min_epochs: 25
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: 0.1
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: 10
  enable_checkpointing: null
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: false
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
verbose: 0
experiment_name: null
transfer_ckpt: null
no_auto_configure_optimizers: false
lr_step_interval: epoch
lr_monitor:
  logging_interval: epoch
  log_momentum: false
  log_weight_decay: false
model_summary:
  max_depth: 2
fit:
  checkpoint_every:
    dirpath: checkpoints
    filename: epoch={epoch}-RMSE={validation/RMSE:.4f}
    monitor: validation/RMSE
    verbose: false
    save_last: null
    save_top_k: -1
    save_weights_only: false
    mode: min
    auto_insert_metric_name: false
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: 50
    save_on_train_epoch_end: null
    enable_version_counter: true
  checkpoint_best:
    dirpath: checkpoints
    filename: epoch={epoch}-RMSE={validation/RMSE:.4f}
    monitor: validation/RMSE
    verbose: false
    save_last: link
    save_top_k: 3
    save_weights_only: false
    mode: min
    auto_insert_metric_name: false
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: null
    save_on_train_epoch_end: null
    enable_version_counter: false
ckpt_path: null
data:
  init_args:
    known_covariates:
    - I_sim_A
    - I_sim_A_dot
    target_covariate: B_sim_eddy_T
    known_past_covariates: null
    train_df_paths:
    - ~/cernbox/hysteresis/dipole/datasets/pretraining/pretrain_train_24h.parquet
    val_df_paths:
    - ~/cernbox/hysteresis/dipole/datasets/pretraining/pretrain_validation_1h.parquet
    normalize: false
    ctxt_seq_len: 300
    tgt_seq_len: 100
    min_ctxt_seq_len: 100
    min_tgt_seq_len: 100
    randomize_seq_len: true
    stride: 1
    downsample: 20
    downsample_method: interval
    target_depends_on: I_sim_A
    time_column: time_ms
    time_format: relative
    extra_transforms:
      I_sim_A:
      - class_path: transformertf.data.RunningNormalizer
        init_args:
          num_features_: 1
          center_: 1820.0
          scale_: 1740.0
          n_samples_seen_: 0.0
          frozen_: true
      I_sim_A_dot:
      - class_path: transformertf.data.RunningNormalizer
        init_args:
          num_features_: 1
          center_: 0.0
          scale_: 1300.0
          n_samples_seen_: 0.0
          frozen_: true
      B_sim_eddy_T:
      - class_path: transformertf.data.DiscreteFunctionTransform
        init_args:
          xs_: ~/cernbox/hysteresis/calibration_fn/SPS_MB_I2B_CALIBRATION_FN_v7.csv
          ys_: null
      - class_path: transformertf.data.RunningNormalizer
        init_args:
          num_features_: 1
          center_: 0.0
          scale_: 0.00103
          n_samples_seen_: 0.0
          frozen_: true
    batch_size: 32
    num_workers: 4
    dtype: float32
    shuffle: true
    distributed: auto
  class_path: transformertf.data.EncoderDecoderDataModule
lr_scheduler:
  class_path: lightning.pytorch.cli.ReduceLROnPlateau
  init_args:
    monitor: validation/RMSE
    mode: min
    factor: 0.5
    patience: 2
    threshold: 0.0001
    threshold_mode: rel
    cooldown: 0
    min_lr: 1.0e-07
    eps: 1.0e-08
    verbose: deprecated
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.00051
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    weight_decay: 0.0001
    amsgrad: false
    maximize: false
    foreach: null
    capturable: false
    differentiable: false
    fused: null
model:
  class_path: transformertf.models.temporal_fusion_transformer.TemporalFusionTransformer
  init_args:
    n_dim_model: 500
    hidden_continuous_dim: 64
    num_heads: 8
    num_lstm_layers: 1
    dropout: 0.222
    output_dim: 7
    criterion:
      class_path: transformertf.nn.QuantileLoss
      init_args:
        quantiles:
        - 0.02
        - 0.1
        - 0.25
        - 0.5
        - 0.75
        - 0.9
        - 0.98
    casual_attention: true
    prediction_type: point
    log_grad_norm: false
    compile_model: true
    trainable_parameters: null
