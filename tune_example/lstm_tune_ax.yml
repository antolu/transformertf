# LSTM hyperparameter tuning with Ax (Meta's adaptive experimentation)
# Ax is great for sample-efficient optimization with advanced modeling

base_config:
  seed_everything: true
  trainer:
    accelerator: auto
    strategy: auto
    devices: auto
    max_epochs: 100
    gradient_clip_val: 1.0
  model:
    class_path: transformertf.models.lstm.LSTM
    init_args:
      num_features: 1
      num_layers: 2
      d_model: 128
      dropout: 0.2
      criterion:
        class_path: transformertf.nn.QuantileLoss
        init_args:
          quantiles: [0.25, 0.5, 0.75]
  data:
    class_path: transformertf.data.TimeSeriesDataModule
    init_args:
      known_covariates: ["I_meas_A_medfilt"]
      target_covariate: "B_meas_T_medfilt"
      train_df_paths: ["~/data/train.parquet"]
      val_df_paths: ["~/data/val.parquet"]
      seq_len: 300
      batch_size: 512
      num_workers: 4

search_space:
  model.init_args.num_layers:
    type: choice
    values: [1, 2, 3, 4]

  model.init_args.d_model:
    type: choice
    values: [64, 128, 256, 512]

  data.init_args.seq_len:
    type: choice
    values: [200, 300, 400, 500]

  optimizer.init_args.lr:
    type: loguniform
    min: 1.e-5
    max: 1.e-2

tune_config:
  num_samples: 30
  metric: "RMSE/validation/dataloader_idx_0"
  mode: min

  logging_metrics:
    - "MSE/validation/dataloader_idx_0"
    - "SMAPE/validation/dataloader_idx_0"
    - "loss/train"

  # Ax search algorithm configuration
  search_algorithm:
    type: ax
    num_bootstrap: 20          # Number of bootstrap samples for uncertainty
    min_trials_observed: 5     # Minimum trials before using model
    verbose_logging: false     # Reduce Ax logging output

  scheduler:
    type: asha
    max_t: 100
    grace_period: 10
    reduction_factor: 2

  experiment_name: "lstm_ax_optimization"
  storage_path: "/tmp/ray_results"
